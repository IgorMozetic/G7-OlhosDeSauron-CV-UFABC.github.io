<!DOCTYPE html>
<html lang="pt-br">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Relatório de Laboratório 3: Câmera Estéreo</title>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$']],
        displayMath: [['$$', '$$']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="../../assets/style.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-4Q6Gf2aSP4eDXB8Miphtr37CMZZQ5oXLH2yaXMJ2w8e2ZtHTl7GptT4jmndRuHDT" crossorigin="anonymous">
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.css" rel="stylesheet" />
  <style>
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 25px 0;
      font-size: 0.9em;
    }

    th,
    td {
      border: 1px solid #bdc3c7;
      padding: 12px;
      text-align: left;
    }

    th {
      background-color: #3498db;
      color: white;
      font-weight: bold;
    }

    tr:nth-child(even) {
      background-color: #f2f2f2;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 20px auto;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 5px;
    }

    figcaption {
      text-align: center;
      font-style: italic;
      color: #7f8c8d;
      margin-top: -10px;
      margin-bottom: 20px;
    }

    .note,
    .best-practices {
      padding: 15px;
      margin: 20px 0;
      border-left: 5px solid;
      background-color: #f9f9f9;
    }

    .note {
      border-color: #f1c40f;
      /* Amarelo */
    }

    .best-practices {
      border-color: #2ecc71;
      /* Verde */
    }

    .katex-display>.katex {
      text-align: center;
    }

    .katex-display {
      overflow-x: auto;
      overflow-y: hidden;
      padding: 1em 0;
    }

    .MathJax {
      font-size: 1.1em !important;
    }

    td .MathJax {
      font-size: 0.9em !important;
    }

    .MathJax_Display {
      margin: 0.5em 0 !important;
    }
  </style>
</head>

<body class="d-flex justify-content-center flex-column align-items-center">
  <header class="py-4 w-100 text-center text-white">
    <h1 class="m-0">Grupo 07: Equipe Olhos de Sauron - CV - Q2.2025</h1>
  </header>

  <main class="py-5 w-100 container relatorio">
    <nav aria-label="breadcrumb">
      <ol class="breadcrumb">
        <li class="breadcrumb-item"><a class="text-black"
            href="https://igormozetic.github.io/G7-OlhosDeSauron-CV-UFABC.github.io/">Home</a></li>
        <li class="breadcrumb-item"><a class="text-black"
            href="https://igormozetic.github.io/G7-OlhosDeSauron-CV-UFABC.github.io/RELATORIOS/">Relatórios</a></li>
      </ol>
    </nav>

    <p class="h3 w-100 text-center"><a href="#" class="text-decoration-none text-black">Relatório de Laboratório 3:
        Câmera Estéreo</a></p>

    <div class="details py-3">
      <p><b>INTEGRANTES:</b></p>
      <p>Igor Domingos da Silva Mozetic - 11202320802</p>
      <p>Jhonattan Ferreira Machado - 11202320245</p>
      <p>Mikael Alves Monteiro - 21055813</p>
      <p><b>Data de Realização do relatório:</b> 14/07/2025</p>
      <p><b>Data de Publicação do relatório:</b> 16/07/2025</p>
      <hr>
    </div>


    <section class="pb-4">
      <p class="h2"><b>INTRODUÇÃO</b></p>

      <p class="py-1">
	  A visão do ser humano é originalmente composta por dois olhos, o que possibilita a paralaxe: diferença na posição aparente de um item com relação a um plano de fundo.
	  É a paralaxe que permite obter a sensação de profundidade, a tridimensionalidade. 
	  Embora a visão monocular até permita um nível de percepção rudimentar de profundidade valendo-se de características específicas do olho e principalmente do ambiente, ela não é equiparável com a visão binocular.
	  Nesse laboratório, são realizadas atividades que exploram a possibilidade computacional de capturar, processar e reproduzir imagens de forma a oferecer a capacidade de sentir a profundidade de itens do cenário.
      </p>
	  
	  <p class="h3"><b>1. Introdução à teoria da Estereoscopia</b></p>
	  
	  <p class="py-1">
	  Serão apresentados, nos próximos parágrafos, conceitos introdutórios de estereoscopia e estratégias de geração de conteúdo tridimensional.
	  </p>
	  
      <p class="h4"><b>1.1. Introdução à Geometria Epipolar e Visão Estéreo</b></p>

      <p class="py-1">
	  Uma imagem bidimensional é uma projeção plana que, por ter apenas duas dimensões, não apresenta um vetor que permita inferir qualquer distância de qualquer item da foto.
	  Se um observador olhar para essa imagem, ao focar em um objeto dela, não terá recebido nenhuma informação de profundidade, mesmo que imprecisa e rudimentar, junto ao feixe individual de luz que entra no olho.
	  Para oferecer a possibilidade de sensação de profundidade em uma imagem, se faz necessário ter mais de uma captura daquele ambiente.</p>

      <p class="py-1">
	  Ao adicionar, num determinado cenário de observação, uma segunda captura devidamente feita em um ângulo ligeiramente diferente no cenário, percebe-se que é possível encontrar um <b>único ponto</b> onde os feixes de luz se cruzam, apresentando um único ponto de convergência.
	  Com os devidos cuidados, podemos aproveitar esse ponto como uma <b>referência de profundidade</b>.</p><!-- Trabalhando aqui-->
	  
	  <p class="py-1"><b><i>Falta: inserir alguma imagem de dois planos com dois raios de luz passando e se encontrando</i></b>
	  </p>
	  
	  <p class="py-1">
	  Essa estratégia de usar duas imagens, onde os raios de luz de cada imagem se cruzam gerando uma intersecção num determinado ponto, é chamada de triangulação.
	  </p>
	  
	  <p class="py-1">
	  <b><i>Continua...</i></b>
	  </p>
	  
	  <p class="h4"><b>1.2. Câmera 3D com OpenCV</b></p>
	  
	  <p class="py-1">De forma geral, os passos para criar um conjunto de câmera estéreo são os seguintes:<br><br>
	  1 - Configuração física das câmeras;<br>
	  2 - Calibração individual das câmeras;<br>
	  3 - Calibração das câmeras (como um conjunto) com parâmetros intrínsecos;<br>
	  4 - Etapas adicionais de correção de distorção.<br>
	  </p>
	  
	  <p class="py-1">
	  <b><i>Continua...</i></b>
	  </p>
	  
	  <!-- Ref de p -->
	  <p class="py-1">
	  </p>
	  
	  <p class="h3"><b>2. Procedimentos experimentais</b></p>
	  <br>
	  
	  <p class="h4"><b>2.1. Construção de câmera estereoscópica simples</b></p>
	  <br>
	  
	  <p class="py-1">
	  Para a parte física do laboratório, será necessário:<br><br>
	  Duas câmeras idênticas;<br>
	  Fita adesiva;<br>
	  Base.<br>
	  </p>
	  
	  <p class="py-1">
	  Instruções adicionais:<br><br>
	  As câmeras devem manter distância de aproximadamente 5cm entre si;<br>
	  Precisam estar bem fixadas (principalmente por conta das etapas após calibração).<br>
	  </p>
	  
	  <p class="py-1">
	  Dica: usar fita dupla-face nas bases das câmeras.<br>
	  Dica: usar um papel marcado, com o propósito de guia, apresentando as posições das bases das câmeras (caso seja necessário refazer o procedimento de <i>setup</i>).
	  </p>
	  
	  <p class="h4"><b>2.2. a) Obtenção dos códigos do exemplo da câmera estéreo com OpenCV</b></p>
	  <br>
	  
	  <p class="py-1"> Para o desenvolvimento desse laboratório, foi utilizado o conteúdo do endereço
	  <a href=https://github.com/spmallick/learnopencv/tree/master/stereo-camera>
	  https://github.com/spmallick/learnopencv/tree/master/stereo-camera</a>
	  </p>
	  
	  <p class="py-1">
	  Tal pasta já contém um conjunto de imagens para fornecer um exemplo de procedimento de calibração e geração de vídeo 3D (para a etapa b).
	  </p>
	  
	  <p class="h5"><b>2.2.1. Código utilizado para calibração - <code>calibrate.py</code></b></p>
	  <br>
	  
	  <div class="d-flex justify-content-center">
        <figure>
          <pre><code class="language-python">
#Código-fonte de calibrate.py, obtido em https://github.com/spmallick/learnopencv/tree/master/stereo-camera
#Na primeira etapa, executar com as imagens fornecidas
import numpy as np 
import cv2
from tqdm import tqdm

# Set the path to the images captured by the left and right cameras
pathL = "./data/stereoL/"
pathR = "./data/stereoR/"

print("Extracting image coordinates of respective 3D pattern ....\n")

# Termination criteria for refining the detected corners
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)


objp = np.zeros((9*6,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

img_ptsL = []
img_ptsR = []
obj_pts = []

for i in tqdm(range(1,28)):
	imgL = cv2.imread(pathL+"img%d.png"%i)
	imgR = cv2.imread(pathR+"img%d.png"%i)
	imgL_gray = cv2.imread(pathL+"img%d.png"%i,0)
	imgR_gray = cv2.imread(pathR+"img%d.png"%i,0)

	outputL = imgL.copy()
	outputR = imgR.copy()

	retR, cornersR =  cv2.findChessboardCorners(outputR,(9,6),None)
	retL, cornersL = cv2.findChessboardCorners(outputL,(9,6),None)

	if retR and retL:
		obj_pts.append(objp)
		cv2.cornerSubPix(imgR_gray,cornersR,(11,11),(-1,-1),criteria)
		cv2.cornerSubPix(imgL_gray,cornersL,(11,11),(-1,-1),criteria)
		cv2.drawChessboardCorners(outputR,(9,6),cornersR,retR)
		cv2.drawChessboardCorners(outputL,(9,6),cornersL,retL)
		cv2.imshow('cornersR',outputR)
		cv2.imshow('cornersL',outputL)
		cv2.waitKey(0)

		img_ptsL.append(cornersL)
		img_ptsR.append(cornersR)


print("Calculating left camera parameters ... ")
# Calibrating left camera
retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(obj_pts,img_ptsL,imgL_gray.shape[::-1],None,None)
hL,wL= imgL_gray.shape[:2]
new_mtxL, roiL= cv2.getOptimalNewCameraMatrix(mtxL,distL,(wL,hL),1,(wL,hL))

print("Calculating right camera parameters ... ")
# Calibrating right camera
retR, mtxR, distR, rvecsR, tvecsR = cv2.calibrateCamera(obj_pts,img_ptsR,imgR_gray.shape[::-1],None,None)
hR,wR= imgR_gray.shape[:2]
new_mtxR, roiR= cv2.getOptimalNewCameraMatrix(mtxR,distR,(wR,hR),1,(wR,hR))


print("Stereo calibration .....")
flags = 0
flags |= cv2.CALIB_FIX_INTRINSIC
# Here we fix the intrinsic camara matrixes so that only Rot, Trns, Emat and Fmat are calculated.
# Hence intrinsic parameters are the same 

criteria_stereo= (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)


# This step is performed to transformation between the two cameras and calculate Essential and Fundamenatl matrix
retS, new_mtxL, distL, new_mtxR, distR, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(obj_pts,
                                                          img_ptsL,
                                                          img_ptsR,
                                                          new_mtxL,
                                                          distL,
                                                          new_mtxR,
                                                          distR,
                                                          imgL_gray.shape[::-1],
                                                          criteria_stereo,
                                                          flags)

# Once we know the transformation between the two cameras we can perform stereo rectification
# StereoRectify function
rectify_scale= 1 # if 0 image croped, if 1 image not croped
rect_l, rect_r, proj_mat_l, proj_mat_r, Q, roiL, roiR= cv2.stereoRectify(new_mtxL, distL, new_mtxR, distR,
                                                 imgL_gray.shape[::-1], Rot, Trns,
                                                 rectify_scale,(0,0))

# Use the rotation matrixes for stereo rectification and camera intrinsics for undistorting the image
# Compute the rectification map (mapping between the original image pixels and 
# their transformed values after applying rectification and undistortion) for left and right camera frames
Left_Stereo_Map= cv2.initUndistortRectifyMap(new_mtxL, distL, rect_l, proj_mat_l,
                                             imgL_gray.shape[::-1], cv2.CV_16SC2)
Right_Stereo_Map= cv2.initUndistortRectifyMap(new_mtxR, distR, rect_r, proj_mat_r,
                                              imgR_gray.shape[::-1], cv2.CV_16SC2)


print("Saving parameters ......")
cv_file = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_WRITE)
cv_file.write("Left_Stereo_Map_x",Left_Stereo_Map[0])
cv_file.write("Left_Stereo_Map_y",Left_Stereo_Map[1])
cv_file.write("Right_Stereo_Map_x",Right_Stereo_Map[0])
cv_file.write("Right_Stereo_Map_y",Right_Stereo_Map[1])
cv_file.release()
		  </code></pre>
          <figcaption class="text-center"><i>Código 1</i>: Código do arquivo <code class="text-black">calibrate.py</code>
          </figcaption>
        </figure>
      </div>
	  
	  <p class="h5"><b>2.2.2. Código utilizado para geração e apresentação de conteúdo 3D - <code>movie3d.py</code></b></p>
	  <br>
	  
	  <div class="d-flex justify-content-center">
        <figure>
          <pre><code class="language-python">
#Código-fonte de movie3d.py, obtido em https://github.com/spmallick/learnopencv/tree/master/stereo-camera
import numpy as np 
import cv2


CamL_id = "data/stereoL.mp4"
CamR_id = "data/stereoR.mp4"

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)

print("Reading parameters ......")
cv_file = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_READ)

Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
Right_Stereo_Map_x = cv_file.getNode("Right_Stereo_Map_x").mat()
Right_Stereo_Map_y = cv_file.getNode("Right_Stereo_Map_y").mat()
cv_file.release()


while True:
	retR, imgR= CamR.read()
	retL, imgL= CamL.read()
	
	if retL and retR:
		imgR_gray = cv2.cvtColor(imgR,cv2.COLOR_BGR2GRAY)
		imgL_gray = cv2.cvtColor(imgL,cv2.COLOR_BGR2GRAY)

		Left_nice= cv2.remap(imgL,Left_Stereo_Map_x,Left_Stereo_Map_y, cv2.INTER_LANCZOS4, cv2.BORDER_CONSTANT, 0)
		Right_nice= cv2.remap(imgR,Right_Stereo_Map_x,Right_Stereo_Map_y, cv2.INTER_LANCZOS4, cv2.BORDER_CONSTANT, 0)

		output = Right_nice.copy()
		output[:,:,0] = Right_nice[:,:,0]
		output[:,:,1] = Right_nice[:,:,1]
		output[:,:,2] = Left_nice[:,:,2]

		# output = Left_nice+Right_nice
		output = cv2.resize(output,(700,700))
		cv2.namedWindow("3D movie",cv2.WINDOW_NORMAL)
		cv2.resizeWindow("3D movie",700,700)
		cv2.imshow("3D movie",output)

		cv2.waitKey(1)
	
	else:
		break

		  </code></pre>
          <figcaption class="text-center"><i>Código 2</i>: Código do arquivo <code class="text-black">movie3d.py</code>
          </figcaption>
        </figure>
      </div>
		  
	  <p class="py-1">
	  <b><i>Continua...</i></b>
	  </p>
	  
	  <p class="h4"><b>2.3. b) Execução dos exemplos com as imagens fornecidas</b></p>
	  <br>
	  
	  <p class="py-1">
	  O diretório indicado no GitHub contém imagens que servem como exemplo para execução dos códigos fornecidos (em <i>/data</i>).
	  São algumas imagens a utilizar com o código <code>calibrate.py</code> e vídeos para uso com <code>movie3d.py</code>
	  </p>
	  
	  <p class="py-1">
	  1) Executar calibração estéreo - <code>calibrate.py</code><br>
	  2) Executar geração e apresentação de imagem 3D - <code>movie3d.py</code>
	  </p>
	  
	  <p class="py-1"><i><h2>Responda:</h2>Consulte a teoria da calibração e correção de distorção e o exemplo
	  executado, e com isso descreva todos os parametros necessários para a camera estéreo.</i>
	  </p>
	  
	  <p class="h4"><b>2.4. c) Calibração do conjunto próprio de câmeras</b></p>
	  <br>
	  
	  <p class="py-1">
	  Com base no laboratório anterior:<br>
	  </p>
	  
	  <p class="py-1">
	  Modificar o código <code>capture_images_abc.py</code> (com base na captura de imagem do laboratório anterior) para obter entre 10 e 15 imagens do padrão do tabuleiro.<br>
	  <i>Observação: usar o nome de um dos integrantes da equipe no nome de cada arquivo a ser gravado.</i>
	  </p>
	  
	  <p class="py-1">
	  Executar a calibração da câmera estéreo com <code>calibrate_abc.py</code> (novo programa com base no código do GitHub para este laboratório).<br>
	  </p>
	  
	  <p class="py-1">
	  Gerar visualização, ao vivo, com base na câmera estéreo da equipe, usando <code>movie3d_abc.py</code> (novo programa com base no código do GitHub para este laboratório).<br>
	  </p>
	  
	  <p class="py-1"><i><h2>Responda:</h2>Liste todos os parâmetros e valores obtidos para sua câmera estéreo,
	  colocando-os nas formas de matrizes e vetores. Quais destes parâmetros forma salvos no arquivo de parâmetros de calibração (xml)?</i>
	  </p>
	  
	  <p class="h4"><b>2.5. d) Geração de vídeo 3D próprio</b></p>
	  <br>
	  
	  <p class="py-1">
	  Alteração do programa do código <code>movie3d.py</code> de tal forma que passe a realizar a gravação de um vídeo 3D para anaglifo (estratégia de uso de cores diferentes para gerar efeito tridimensional), 
	  com aproximadamente 10s a 20s. Usar o exemplo de gravação da aula de laboratório 1. Conversão do video obtido para MP4.
	  </p>
	  
	  <p class="py-1"><i><h2>Responda:</h2>A percepção individual de todos integrantes da equipe. Compare e 
	  analise as diferenças com os resultados ao vivo e gravado.</i>
	  </p>
	  
	  <p class="h3"><b>3. Análise de resultados</b></p>
	  <br>
	  
	  <p class="py-1">
	  <b><i>Continua...</i></b>
	  </p>
	  
	  <p class="h3"><b>4. Conclusão</b></p>
	  <br>
	  
	  <p class="py-1">
	  <b><i>Continua...</i></b>
	  </p>
	  
      <p class="h3"><b>5. Referências</b></p>

      <ol>
        <li>LearnOpenCV. (2020). <em>Introduction to Epipolar Geometry and Stereo Vision</em>. Disponível em:
          <code>https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/</code>
        </li>
        <li>LearnOpenCV. (2021). <em>Making A Low-Cost Stereo Camera Using OpenCV</em>. Disponível em:
          <code>https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/</code>
        </li>
		<li>
		<b>E mais</b>...
		</li>

      </ol>


  </main>

  <footer class="mt-5 p-3">
    &copy; 2025 - Desenvolvido por Equipe Olho de Sauron
  </footer>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>

</html>